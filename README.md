**Project Overview:**

This project focusses on how KPI correlations can be misleading in some dashboards regarding product performance, therefore KPI misaligments and conflicts can pose a risk while evaluating customer loyalty and making strategic decisions regarding which demographics to target. The dataset used is "FinTech Customer Lifetime Value (LTV) Dataset” (7,000+ records) downloaded from Kaggle. The project is under progress, with detailed descriptions added below to explain the project's focus. There is also an initial draft of the project writeup added, which is yet to be cited and developed into a full literature review using real world business use cases. The appendices section is already added to showcase the results from the code blocks. 

**Development:**

This was done without iterative drafts since I had already covered the core Statistics concepts used in this project, instead the code was broken down into five separate scripts with explanations behind each code block included within the codes as well as as elaborate explanations given below, for learning and project tracking purposes. There is a sixth file currently under development, including a PCA code to simplify the data and reduce noise to uncover the most crucial KPI drivers: 

**File 1:** For the first code file, I added a data cleaning code for revision purposes despite the data being in a cleaned format, with a detailed breakdown of all steps included in the first code file.

**File 2:** The second script sets the stage for all downstream analysis by summarizing key KPIs across the entire dataset and then breaking them down by income level. This was a crucial step in the paper, as it helped identify early contradictions in assumed user value. By comparing median values and standard deviations across low-, middle-, and high-income groups, this code revealed that some commonly trusted indicators—like transaction volume or average transaction value—behave differently when viewed through the lens of financial segmentation. The insights here laid the foundation for uncovering KPI conflicts, particularly highlighting how middle-income users, rather than high-income ones, often demonstrated higher lifetime value and overall spend. These findings were essential for supporting the argument that engagement metrics alone don't paint a full picture of user profitability or product success. 

**File 3:** In the third code file, the focus shifts from segmented summaries to individual user behavior. This script was designed to visualize how certain KPIs—such as usage frequency, satisfaction score, and transaction recency—correlate with core financial outcomes like LTV and total spend. Through the generation of scatterplots, this code exposed the weak and inconsistent relationships between these seemingly important engagement metrics and actual user value. These visuals served as strong evidence for the paper’s broader argument: that many dashboard KPIs may look promising on the surface but ultimately fail to predict real-world outcomes when examined user by user. In other words, the scatterplots helped bring the idea of Goodhart’s Law to life—illustrating how a KPI loses its reliability once it becomes the target of performance optimization.

**File 4:** The fourth script digs deeper by stress-testing the stability of KPI relationships across different types of users. It segments the dataset into high-engagement and low-engagement groups based on app usage frequency and then calculates correlation matrices within each group. This step was included in the paper to address a major blind spot in typical KPI analysis: the assumption that relationships between variables are universal. Instead, what this code shows is that engagement level significantly alters how KPIs relate to one another—sometimes flipping the direction or weakening the strength of those relationships entirely. It reinforces the idea that dashboard logic cannot be static, and that interpreting KPIs without context can lead product teams to make poorly informed decisions based on unstable metrics.

**File 5:** The last code was developed to statistically validate—or disprove—some of the intuitive assumptions that emerged from earlier visual and descriptive analyses. This script applies a t-test to compare the satisfaction scores between users with high and low lifetime value, and runs a one-way ANOVA to assess whether income level significantly affects LTV. In both cases, the results revealed no statistically significant difference. These findings confirmed that even the most widely held beliefs—like happier or wealthier users being more valuable—don’t hold up under rigorous testing. This statistical validation was key to reinforcing the overall message of the paper that relying on assumptions or surface-level trends in KPI dashboards can be not only misleading, but actively harmful to strategic decision-making.
